# LM Studio Context Size Benchmark Configuration

# API Settings
api:
  url: "http://localhost:5002"
  timeout: 600  # seconds
  delay_between_requests: 2  # seconds
  delay_between_models: 10  # seconds

# System Information
system:
  name: "M3 Max MacBook Pro 128GB RAM"
  notes: "4-bit quantization, 8-bit KV-Cache"

# Models to test (in order)
models:
  - name: "qwen/qwen3-next-80b"
    enabled: true
    description: "Qwen 3 Next 80B"
    
  - name: "openai/gpt-oss-20b"
    enabled: true
    description: "GPT-OSS 20B"
    
  - name: "openai/gpt-oss-120b"
    enabled: true
    description: "GPT-OSS 120B"

# Test Configuration
test:
  # Context sizes to test (in tokens) - Quick experiment up to 20k
  context_sizes: [1000, 10000, 20000]
  
  # Alternative: Use range instead of explicit list
  # context_range:
  #   start: 10000
  #   end: 100000
  #   step: 10000
  
  # Number of completion tokens to generate for each test
  max_tokens: 512
  
  # Temperature for generation
  temperature: 0.1

# Book content settings
content:
  book_path: "books/harrypotter.pdf"  # Using Harry Potter for this experiment
  # Prompt types to use (random selection)
  prompt_types:
    - "literary_analysis"
    - "creative_writing_feedback" 
    - "editorial_review"
    - "lecture_preparation"
    - "adaptation_analysis"

# Output settings
output:
  results_dir: "results"
  create_charts: true
  save_summary: true
  
# Chart settings
charts:
  dpi: 300
  figure_size: [16, 8]  # width, height in inches
  colors:
    "qwen/qwen3-next-80b": "#d62728"      # Red
    "openai/gpt-oss-20b": "#ff7f0e"       # Orange  
    "openai/gpt-oss-120b": "#2ca02c"      # Green
    "default": "#1f77b4"                  # Blue
